DEVICE          : cuda              # device used for training and evaluation (cpu, cuda, cuda0, cuda1, ...)
SAVE_DIR        : 'output'          # output folder name used for saving the model, logs and inference results

# MODEL:
#   NAME          : CMNeXt                                            # name of the model you are using
#   BACKBONE      : CMNeXt-B2                                       # model variant
#   PRETRAINED    : '/home/chagall/Weights/mit/mit_b2.pth'     # backbone model's weight 
#   RESUME        : ''                                                # checkpoint file

# MODEL:
#   NAME          : MultiModalSharedPrivateNet
#   BACKBONE      : mmsp-tiny             
#   PRETRAINED    : '/home/chagall/Weights/swin/swin_tiny_patch4_window7_224.pth'
#   RESUME        : ''                

# MODEL:
#   NAME          : DiffModalSegV1
#   BACKBONE      : v1-tiny             
#   PRETRAINED    : '/home/chagall/Weights/swin/swin_tiny_patch4_window7_224.pth'
#   RESUME        : ''                

MODEL:
  NAME          : DiffModalSegV3
  BACKBONE      : v3-512-new-tiny             
  PRETRAINED    : '/home/chagall/Weights/swin/swin_tiny_patch4_window7_224.pth'
  RESUME        : ''

DATASET:
  NAME          : MCubeS                                          # dataset name to be trained with (camvid, cityscapes, ade20k)
  ROOT          : '/home/chagall/Datasets/multimodal_dataset'                # dataset root path
  IGNORE_LABEL  : 255
  # MODALS        : ['image'] # 
  # MODALS        : ['image', 'aolp']
  # MODALS        : ['image', 'aolp', 'dolp']
  MODALS        : ['image', 'aolp', 'dolp', 'nir']

TRAIN:
  IMAGE_SIZE    : [512, 512]      # training image size in (h, w) === Fixed in dataloader, following MCubeSNet
  BATCH_SIZE    : 16    
             # batch size used to train 3= 22gb on 4090
  EPOCHS        : 1000             # number of epochs to train
  EVAL_START    : 300            # evaluation interval during training
  EVAL_INTERVAL : 1               # evaluation interval during training
  AMP           : true           # use AMP in training
  DDP           : true            # use DDP training

LOSS:
  NAME          : OhemCrossEntropy     # loss function name
  CLS_WEIGHTS   : false            # use class weights in loss calculation

OPTIMIZER:
  NAME          : adamw           # optimizer name
  LR            : 0.00006         # initial learning rate used in optimizer
  WEIGHT_DECAY  : 0.01            # decay rate used in optimizer 

SCHEDULER:
  NAME          : warmuppolylr    # scheduler name
  POWER         : 0.9             # scheduler power
  WARMUP        : 10              # warmup epochs used in scheduler
  WARMUP_RATIO  : 0.1             # warmup ratio
  

EVAL:
  # MODEL_PATH    : 'output/MCubeS/cmnext_b2_mcubes_rgb.pth'
  # MODEL_PATH    : 'output/MCubeS/cmnext_b2_mcubes_rgba.pth'
  # MODEL_PATH    : 'output/MCubeS/cmnext_b2_mcubes_rgbad.pth'
  # MODEL_PATH    : '/home/chagall/research/Weights/cmnext/cmnext_b2_mcubes_rgbadn.pth'
  MODEL_PATH    : "/home/chagall/research/DELIVER/output/MCubeS_CMNeXt-B2_iadn/CMNeXt_CMNeXt-B2_MCubeS_epoch486_47.93.pth"
  IMAGE_SIZE    : [1024, 1024]    # evaluation image size in (h, w)                       
  BATCH_SIZE    : 12               # batch size used to train
  MSF: 
    ENABLE      : false                                   # multi-scale and flip evaluation  
    FLIP        : true                                    # use flip in evaluation  
    SCALES      : [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]       # scales used in MSF evaluation                
